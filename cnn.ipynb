{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFsdW2NRb_RL"
   },
   "source": [
    "#This notebook contains the CNN used by Team Phlyers for the RMI shared task at VarDial2020. It's an adaptation of the CNN presented in Butnaru and Ionescu (2019), the paper in which the MOROCO corpus was first presented.\n",
    "\n",
    "#The first few blocks are needed to set up the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ElNfPjqpepZ9"
   },
   "outputs": [],
   "source": [
    "# Load the Drive helper and mount\n",
    "from google.colab import drive\n",
    "\n",
    "# This will prompt for authorization.\n",
    "drive.mount('/content/drive')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MCKCET-8fdjB"
   },
   "outputs": [],
   "source": [
    "%cd /content/drive/My Drive/Colab Notebooks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8HAFhRAkfmQ4"
   },
   "source": [
    "#This block loads the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CEIXZLAPb_RO"
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset, IterableDataset\n",
    "from sklearn.metrics import f1_score\n",
    "import string \n",
    "\n",
    "all_categories = []\n",
    "training = []\n",
    "\n",
    "# Load the data and preprocess\n",
    "for line in open('data/train.txt', encoding='utf-8', errors='ignore'):\n",
    "    category = line.split('\\t')[-1].rstrip().strip('\\u202c')\n",
    "    all_categories.append(category)\n",
    "    sentence = line.split('\\t')[0].replace('$NE$', '').lower()\n",
    "    training.append(sentence)\n",
    "\n",
    "# These are letters that appear more than 50 times in the corpus. The others are excluded.\n",
    "all_letters = 'cumaspnetfârşidvoljgzţăbîxwhșțkyкуинсайдертябгqхоéàпвылшǎцáзфьмжщчãöü̦̆ю̧ȋэç'\n",
    "\n",
    "n_letters = len(all_letters)\n",
    "print('Characters:', all_letters)\n",
    "print('# of characters:', len(all_letters))\n",
    "print('# of sentences:', len(training))\n",
    "print('# of labels:', len(set(all_categories)))\n",
    "\n",
    "# Map the characters into a list of indeces, that you use to create the tensors\n",
    "dic_letters = dict(zip(all_letters, range(1, n_letters+1)))\n",
    "\n",
    "\n",
    "#Hyper-parameters\n",
    "\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 20\n",
    "batch_size = 10\n",
    "\n",
    "\n",
    "\n",
    "# Helper functions\n",
    "\n",
    "# Get the letter index\n",
    "def letterToIndex(letter):\n",
    "    if letter in dic_letters:\n",
    "        return dic_letters[letter]\n",
    "    return 0\n",
    "\n",
    "# One-hot vectors\n",
    "\n",
    "#Turns a single line into a tensor\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(n_letters+1, 5000)\n",
    "    for li, letter in enumerate(line[:5000]):\n",
    "        tensor[letterToIndex(letter)][li] = 1 \n",
    "    return tensor\n",
    "\n",
    "#Turns an batch of lines into a batch of tensors\n",
    "def linesToTensors(lines):\n",
    "    tensor = torch.zeros(batch_size, n_letters+1, 5000)\n",
    "    for batch, line in enumerate(lines):\n",
    "      for li, letter in enumerate(line[:5000]):\n",
    "          tensor[batch][letterToIndex(letter)][li] = 1 \n",
    "    return tensor\n",
    "\n",
    "#Turns categories it tensors\n",
    "def categoriesToTensors(categories):\n",
    "    labels = torch.zeros(len(categories),dtype=torch.long)\n",
    "    for i, label in enumerate(categories):\n",
    "      labels[i] = all_categories.index(label)\n",
    "    return labels\n",
    "\n",
    "#Turns tensors into labels\n",
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.data.topk(1) # Tensor out of Variable with .data\n",
    "    return all_categories[top_i[0]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ANPXYHGUGb8"
   },
   "source": [
    "#This is a class based on the DataLoader class that we will use to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "p_9dXnPyZBs1"
   },
   "outputs": [],
   "source": [
    "class MyClass(Dataset):\n",
    "    def __init__(self, training, labels):\n",
    "        self.training = training\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.training)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.training[idx], self.labels[idx])\n",
    "\n",
    "dataset = MyClass(training, all_categories)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hixbQPqpb_Rd"
   },
   "source": [
    "#This is the CNN adapted from Butnaru and Ionescu (2019)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l7GUYQzQb_Rh"
   },
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Creating the Network\n",
    "# ====================\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Sequential(nn.Conv1d(77, 128, kernel_size=7), nn.Threshold(threshold=0.000001, value=0), nn.MaxPool1d(3, stride=3))\n",
    "        self.conv2 = nn.Sequential(nn.Conv1d(128, 128, kernel_size=7), nn.Threshold(threshold=0.000001, value=0), nn.MaxPool1d(3, stride=3))\n",
    "        self.conv3 = nn.Sequential(nn.Conv1d(128, 128, kernel_size=3), nn.Threshold(threshold=0.000001, value=0), nn.MaxPool1d(3, stride=3))\n",
    "        self.fc1 = nn.Sequential(nn.Linear(23424, 1000), nn.Threshold(threshold=0.000001, value=0) ,nn.Dropout())\n",
    "        #For these two layers they do not specify the size. This was fine-tuned by us.\n",
    "        self.fc2 = nn.Sequential(nn.Linear(1000, 1000), nn.Threshold(threshold=0.000001, value=0) ,nn.Dropout())\n",
    "        self.fc3 = nn.Linear(1000, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = self.conv3(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        return softmax(out)\n",
    "\n",
    "model = Net()\n",
    "model = model.to('cuda')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print('The CNN is ready.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jV5L6uslb_Rs"
   },
   "source": [
    "#This is the training phase. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wYGy4JbHb_Rt"
   },
   "outputs": [],
   "source": [
    "total_step = len(training)\n",
    "loss_list = []\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "  predicted_labels = []\n",
    "  correct_labels = []\n",
    "\n",
    "  for i, (sentences, category) in enumerate(dataloader):\n",
    "    tensors = linesToTensors(sentences)\n",
    "    tensors = tensors.to(\"cuda\")\n",
    "    labels = categoriesToTensors(category)\n",
    "    labels = labels.to(\"cuda\")\n",
    "\n",
    "    outputs = model(tensors)\n",
    "    outputs = outputs.to(\"cuda\")\n",
    "\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss_list.append(loss.item())\n",
    "\n",
    "    # Backprop and perform Adam optimisation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Track the training accuracy\n",
    "    total = labels.size(0)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    correct = (predicted == labels).sum().item()\n",
    "    acc = correct / total\n",
    "\n",
    "    predicted_labels.extend([int(label) for label in predicted])\n",
    "    correct_labels.extend([int(label) for label in labels])\n",
    "\n",
    "  print('Training. Epoch-', epoch, 'F-score:', f1_score(predicted_labels, correct_labels, average='macro'))\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), 'model_saved')\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n7bLYPK9ojqm"
   },
   "source": [
    "#Results on the development set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vnRjdQIkorkX"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import string \n",
    "\n",
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.data.topk(1) # Tensor out of Variable with .data\n",
    "    return all_categories[top_i[0]]\n",
    "\n",
    "model.load_state_dict(torch.load('trained_cnn_model'))\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "dev = []\n",
    "\n",
    "# Load the data and preprocess\n",
    "for line in open('data/dev-source.txt'):\n",
    "    category = line.split('\\t')[-1].rstrip()\n",
    "    sentence = line.split('\\t')[0].replace('$NE$', '').lower().replace('FOTO', '').replace('VIDEO','').replace('LIVE','')\n",
    "    dev.append((sentence, category))\n",
    "\n",
    "predicted = []\n",
    "correct = []\n",
    "\n",
    "total = len(dev)\n",
    "\n",
    "for i, (sentence, category) in enumerate(dev):\n",
    "    tensor = torch.reshape(lineToTensor(sentence), (1, n_letters+1, 5000))\n",
    "    tensor = tensor.to(\"cuda\")\n",
    "    outputs = model(tensor)\n",
    "    outputs = outputs.to(\"cpu\")\n",
    "\n",
    "    label = Variable(torch.LongTensor([all_categories.index(category)]))\n",
    "\n",
    "    _, prediction = torch.max(outputs.data, 1)\n",
    "    predicted.append(prediction)\n",
    "    correct.append(label)\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print(i, 'out of', total, 'F-score:', f1_score(predicted,correct, average=\"macro\"))\n",
    "\n",
    "print(f1_score(predicted,correct, average=\"macro\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "colab": {
   "name": "cnn.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
