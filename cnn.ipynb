{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "cnn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFsdW2NRb_RL"
      },
      "source": [
        "#This notebook contains the CNN used by Team Phlyers for the RMI shared task at VarDial2020. It's an adaptation of the CNN presented in Butnaru and Ionescu (2019), the paper in which the MOROCO corpus was first presented.\n",
        "\n",
        "#The first few blocks are needed to set up the directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElNfPjqpepZ9"
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCKCET-8fdjB"
      },
      "source": [
        "%cd /content/drive/My Drive/\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HAFhRAkfmQ4"
      },
      "source": [
        "#This block loads the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEIXZLAPb_RO"
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "from collections import Counter\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader, Dataset, IterableDataset\n",
        "from sklearn.metrics import f1_score\n",
        "import string \n",
        "\n",
        "\n",
        "'''\n",
        "The code has been adapted from:\n",
        "\n",
        "Convolutional Neural Networks Tutorial in PyTorch \n",
        "(https://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-in-pytorch/)\n",
        "\n",
        "NLP FROM SCRATCH: CLASSIFYING NAMES WITH A CHARACTER-LEVEL RNN\n",
        "by Sean Robertson\n",
        "https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html\n",
        "'''\n",
        "\n",
        "##############################\n",
        "# Load the data and preprocess\n",
        "##############################\n",
        "\n",
        "labels = []\n",
        "sentences = []\n",
        "\n",
        "for line in open('data/train.txt', encoding='utf-8', errors='ignore'):\n",
        "    category = line.split('\\t')[-1].rstrip().strip('\\u202c')\n",
        "    labels.append(category)\n",
        "    sentence = line.split('\\t')[0].replace('$NE$', '').lower()\n",
        "    sentences.append(sentence)\n",
        "\n",
        "label_list = list(set(labels))\n",
        "\n",
        "# These are letters that appear more than 50 times in the corpus. The others are excluded.\n",
        "all_letters = 'cumaspnetfârşidvoljgzţăbîxwhșțkyкуинсайдертябгqхоéàпвылшǎцáзфьмжщчãöü̦̆ю̧ȋэç'\n",
        "\n",
        "# Print size of the corpus\n",
        "\n",
        "n_letters = len(all_letters)\n",
        "print('Characters:', all_letters)\n",
        "print('# of characters:', len(all_letters))\n",
        "print('# of sentences:', len(sentences))\n",
        "print('# of labels:', len(label_list))\n",
        "\n",
        "# Map the characters into a list of indeces, that you use to create the tensors\n",
        "dic_letters = dict(zip(all_letters, range(1, n_letters+1)))\n",
        "\n",
        "\n",
        "##################\n",
        "#Hyper-parameters\n",
        "##################\n",
        "\n",
        "learning_rate = 0.0001\n",
        "num_epochs = 20\n",
        "batch_size = 10\n",
        "\n",
        "\n",
        "###################\n",
        "# Helper functions\n",
        "###################\n",
        "\n",
        "# Get the index of the letter\n",
        "def letterToIndex(letter):\n",
        "    if letter in dic_letters:\n",
        "        return dic_letters[letter]\n",
        "    return 0\n",
        "\n",
        "#Turns a single line into a tensor\n",
        "def lineToTensor(line):\n",
        "    tensor = torch.zeros(n_letters+1, 5000)\n",
        "    for li, letter in enumerate(line[:5000]):\n",
        "        tensor[letterToIndex(letter)][li] = 1 \n",
        "    return tensor\n",
        "\n",
        "#Turns an batch of lines into a batch of tensors\n",
        "def linesToTensors(lines):\n",
        "    tensor = torch.zeros(batch_size, n_letters+1, 5000)\n",
        "    for batch, line in enumerate(lines):\n",
        "      for li, letter in enumerate(line[:5000]):\n",
        "          tensor[batch][letterToIndex(letter)][li] = 1 \n",
        "    return tensor\n",
        "\n",
        "#Turns categories it tensors\n",
        "def categoriesToTensors(categories):\n",
        "    labels = torch.zeros(len(categories),dtype=torch.long)\n",
        "    for i, label in enumerate(categories):\n",
        "      labels[i] = label_list.index(label)\n",
        "    return labels\n",
        "\n",
        "#Turns tensors into labels\n",
        "def categoryFromOutput(output):\n",
        "    top_n, top_i = output.data.topk(1) # Tensor out of Variable with .data\n",
        "    return label_list[top_i[0]]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ANPXYHGUGb8"
      },
      "source": [
        "#This is a class based on the DataLoader class that we will use to load the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_9dXnPyZBs1"
      },
      "source": [
        "class MyClass(Dataset):\n",
        "    def __init__(self, training, labels):\n",
        "        self.training = training\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.training)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (self.training[idx], self.labels[idx])\n",
        "\n",
        "dataset = MyClass(sentences, labels)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hixbQPqpb_Rd"
      },
      "source": [
        "#This is the CNN adapted from Butnaru and Ionescu (2019)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7GUYQzQb_Rh"
      },
      "source": [
        "######################################################################\n",
        "# Creating the Network\n",
        "# ====================\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Sequential(nn.Conv1d(77, 128, kernel_size=7), nn.Threshold(threshold=0.000001, value=0), nn.MaxPool1d(3, stride=3))\n",
        "        self.conv2 = nn.Sequential(nn.Conv1d(128, 128, kernel_size=7), nn.Threshold(threshold=0.000001, value=0), nn.MaxPool1d(3, stride=3))\n",
        "        self.conv3 = nn.Sequential(nn.Conv1d(128, 128, kernel_size=3), nn.Threshold(threshold=0.000001, value=0), nn.MaxPool1d(3, stride=3))\n",
        "        self.fc1 = nn.Sequential(nn.Linear(23424, 1000), nn.Threshold(threshold=0.000001, value=0) ,nn.Dropout())\n",
        "        #For these two layers they do not specify the size. This was fine-tuned by us.\n",
        "        self.fc2 = nn.Sequential(nn.Linear(1000, 1000), nn.Threshold(threshold=0.000001, value=0) ,nn.Dropout())\n",
        "        self.fc3 = nn.Linear(1000, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.conv2(out)\n",
        "        out = self.conv3(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.fc3(out)\n",
        "        softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        return softmax(out)\n",
        "\n",
        "model = Net()\n",
        "model = model.to('cuda')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print('The CNN is ready.')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jV5L6uslb_Rs"
      },
      "source": [
        "#This is the training phase. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYGy4JbHb_Rt"
      },
      "source": [
        "total_step = len(sentences)\n",
        "loss_list = []\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "  predicted_labels = []\n",
        "  correct_labels = []\n",
        "\n",
        "  for i, (lines, categories) in enumerate(dataloader):\n",
        "    tensors = linesToTensors(lines)\n",
        "    tensors = tensors.to(\"cuda\")\n",
        "    label_tensors = categoriesToTensors(categories)\n",
        "    label_tensors = label_tensors.to(\"cuda\")\n",
        "\n",
        "    outputs = model(tensors)\n",
        "    outputs = outputs.to(\"cuda\")\n",
        "\n",
        "    loss = criterion(outputs, label_tensors)\n",
        "    loss_list.append(loss.item())\n",
        "\n",
        "    # Backprop and perform Adam optimisation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Track the training accuracy\n",
        "    total = label_tensors.size(0)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    correct = (predicted == label_tensors).sum().item()\n",
        "    acc = correct / total\n",
        "\n",
        "    predicted_labels.extend([int(label) for label in predicted])\n",
        "    correct_labels.extend([int(label) for label in label_tensors])\n",
        "\n",
        "  print('Training. Epoch-', epoch, 'F-score:', f1_score(predicted_labels, correct_labels, average='macro'))\n",
        "\n",
        "\n",
        "torch.save(model.state_dict(), 'trained_cnn_model')\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7bLYPK9ojqm"
      },
      "source": [
        "#Results on the development set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnRjdQIkorkX"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import string \n",
        "\n",
        "# Load the model\n",
        "model.load_state_dict(torch.load('trained_cnn_model'))\n",
        "model = model.to(\"cuda\")\n",
        "\n",
        "# Load the data and preprocess\n",
        "dev = []\n",
        "\n",
        "for line in open('data/dev-source.txt'):\n",
        "    category = line.split('\\t')[-1].rstrip()\n",
        "    sentence = line.split('\\t')[0].replace('$NE$', '').lower().replace('FOTO', '').replace('VIDEO','').replace('LIVE','')\n",
        "    dev.append((sentence, category))\n",
        "\n",
        "# Test the network on the development set\n",
        "predicted = []\n",
        "correct = []\n",
        "\n",
        "total = len(dev)\n",
        "\n",
        "for i, (sentence, category) in enumerate(dev):\n",
        "    tensor = torch.reshape(lineToTensor(sentence), (1, n_letters+1, 5000))\n",
        "    tensor = tensor.to(\"cuda\")\n",
        "    outputs = model(tensor)\n",
        "    outputs = outputs.to(\"cpu\")\n",
        "\n",
        "    label = Variable(torch.LongTensor([label_list.index(category)]))\n",
        "\n",
        "    _, prediction = torch.max(outputs.data, 1)\n",
        "    predicted.append(prediction)\n",
        "    correct.append(label)\n",
        "\n",
        "    if i % 1000 == 0:\n",
        "        print(i, 'out of', total, 'F-score:', f1_score(predicted,correct, average=\"macro\"))\n",
        "\n",
        "print(f1_score(predicted,correct, average=\"macro\"))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}